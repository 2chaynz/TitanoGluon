# TitanoGluon

My submission for the **Titanic Challenge** on Kaggle, where I explored two ML models: **AutoGluon** and **Random Forest**.

-  **Best result:** 0.78947 on Kaggle with AutoGluon  
-  **Rank:** 1303rd out of 14,661 participants
- Data preprocessing, feature engineering, various encoding techniques, and hyperparameter optimization.-
- Includes: commented scripts, outputs, CSV files, and a **Typst** document summarizing my notes and learnings.

---

<p align="center">
  <img src="https://github.com/user-attachments/assets/1f268aff-2742-4c6a-a4f7-c141956fa38d" alt="Titanic Challenge Results" width="450">
</p>
<p align="center"><em>HTML output of performances from the AutoGluon script</em></p>

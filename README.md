# TitanoGluon

My submission for the **Titanic Challenge** on Kaggle, where I explored two ML models: **AutoGluon** and **Random Forest**.

- Best result: Using AutoGluonâ€™s *best quality* mode â†’ **0.890514** (ROC-AUC metric) and **0.78947** on Kaggle (vs **0.76315** with the *medium* mode)  
- **Rank:** 1303rd out of 14,661 participants  
- Process: Data preprocessing, feature engineering, various encoding techniques, and hyperparameter optimization  
- Includes commented scripts, outputs, CSV files, and a Typst document summarizing my notes and learnings  

### Note  
The included PDF is in French ðŸ‡«ðŸ‡· (my bad â€” it was for personal use).  

---

<p align="center">
  <img src="https://github.com/user-attachments/assets/1f268aff-2742-4c6a-a4f7-c141956fa38d" alt="Titanic Challenge Results" width="450">
</p>
<p align="center"><em>HTML output of performances from the AutoGluon script</em></p>

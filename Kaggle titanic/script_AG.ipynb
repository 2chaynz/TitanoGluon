{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Titanic Kaggle Challenge - Auto-Gluon\n",
    "\n",
    "For detailed explanations please refer to the Typst documentation file in the GitHub repository.\n",
    "\"\"\"\n",
    "\n",
    "# Summary:\n",
    "# The file should have exactly 2 columns:\n",
    "# PassengerId (sorted in any order)\n",
    "# Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n",
    "\n",
    "############################ INSTALLATION ############################\n",
    "\n",
    "# !pip install autogluon  # If you don't have AutoGluon installed (even on Google Colab)\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "############################ CONFIG & TRAINING ############################\n",
    "\n",
    "TARGET_COLUMN = 'Survived'\n",
    "\n",
    "'''\n",
    "Here it’s a basic configuration for quickly testing AutoGluon: “medium-quality”.\n",
    "About the metric used hre : For Binary classification (Survived: 1, else: 0), roc_auc is the best metric.\n",
    "---> See notes in the typst file.\n",
    "\n",
    "NB :\n",
    "Moreover in the log it said:\n",
    "If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init\n",
    "(You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
    "'''\n",
    "\n",
    "predictor = TabularPredictor(   # predictor = model, just like RandomForestRegressor etc., here only defined\n",
    "    label=TARGET_COLUMN,\n",
    "    eval_metric='roc_auc',\n",
    "    path='./autogluon_models'   # Save directory\n",
    ").fit(                          # Fit = train the model\n",
    "    train_data=train_data,\n",
    "    time_limit=600,\n",
    "    presets='medium_quality'    # Options: 'best_quality', 'high_quality', 'good_quality', 'medium_quality'\n",
    ")\n",
    "\n",
    "'''\n",
    "About path:\n",
    "When AutoGluon trains your models, it creates a folder containing:\n",
    "\n",
    "./autogluon_models/\n",
    "├── models/\n",
    "│   ├── LightGBM/             # The trained LightGBM model\n",
    "│   ├── CatBoost/             # The trained CatBoost model\n",
    "│   ├── NeuralNetTorch/       # The trained neural network\n",
    "│   ├── WeightedEnsemble_L2/  # The ensemble model (combination)\n",
    "│   └── ...                   # All other models\n",
    "├── utils/\n",
    "│   └── feature_generator.pkl # Data preprocessing transformations\n",
    "├── predictor.pkl             # Predictor configuration\n",
    "└── SummaryOfModels.html      # HTML summary report\n",
    "\n",
    "With:\n",
    "path='./autogluon_models'\n",
    "     ││  │\n",
    "     ││  └─ Folder name (you can change it)\n",
    "     │└─── Current directory (where the code runs)\n",
    "     └──── Relative path\n",
    "'''\n",
    "\n",
    "\n",
    "############################ RESULTS & ANALYSIS FOR MEDIUM QUALITY ############################\n",
    "\n",
    "leaderboard = predictor.leaderboard(train_data, silent=True)\n",
    "# Creates a DataFrame ranking all models from best to worst with their scores, training time, etc.\n",
    "# This is a built-in AutoGluon function.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL LEADERBOARD\")\n",
    "print(\"=\"*60)\n",
    "print(leaderboard)\n",
    "\n",
    "# Feature importance\n",
    "# ---> Computes a score for each column (Age, Sex, Pclass...) showing its importance for predictions.\n",
    "# The higher the score, the more important the feature.\n",
    "\n",
    "feature_importance = predictor.feature_importance(train_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Information about the best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL:\", predictor.model_best)\n",
    "print(\"=\"*60)\n",
    "# Example of output: BEST MODEL: WeightedEnsemble_L2\n",
    "\n",
    "# Training summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(predictor.fit_summary())\n",
    "\n",
    "\"\"\"\n",
    "fit_summary() returns a complete summary of the training process.\n",
    "It contains: number of models, total time, best score, etc.\n",
    "Displays a Python dictionary with all training information. ( JSON :) )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ PREDICTIONS ############################\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "predictions = predictor.predict(test_data)  # Stores results (0 or 1 for each passenger)\n",
    "print(\"\\nPredictions:\", predictions.head())\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False) #Download the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ ADVANCED AUTOGLUON ############################\n",
    "\n",
    "'''\n",
    "# For deeper training :\n",
    "\n",
    "predictor_advanced = TabularPredictor(\n",
    "    label=TARGET_COLUMN,\n",
    "    eval_metric='roc_auc',  # Optimal for binary classification\n",
    "    path='./autogluon_models_advanced'\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    time_limit=3600,  # 1h: AutoGluon will optimize many parameters for maximum quality\n",
    "    presets='best_quality',\n",
    "\n",
    "    # Optional custom hyperparameters\n",
    "    hyperparameters={\n",
    "        'GBM': {},        # LightGBM: Gradient Boosting Machine\n",
    "        'CAT': {},        # CatBoost: good for categorical data\n",
    "        'XGB': {},        # XGBoost\n",
    "        'RF': {},         # Random Forest\n",
    "        'NN_TORCH': {},   # PyTorch neural network\n",
    "    },\n",
    "    # IMPORTANT -----> This tells AutoGluon to train these 5 model types with their default hyperparameters.\n",
    "    # -----> It does NOT explore all possible hyperparameter combinations (contrary to what I thought).\n",
    "\n",
    "    # To automatically explore different combinations, enable hyperparameter tuning, e.g.:\n",
    "    # predictor.fit(train_data, hyperparameter_tune_kwargs='auto',)\n",
    ")\n",
    "\n",
    "# That enables automatic hyperparameter search (classic AutoML).\n",
    "\n",
    "\n",
    "## Cross-validation for more robustness\n",
    "# -----> See notes about bagging, stacking, etc.\n",
    "\n",
    "    num_bag_folds=5,     # 5-fold bagging\n",
    "    num_bag_sets=1,      # Usually left as 1\n",
    "    num_stack_levels=1,  # Model stacking\n",
    "\n",
    "    # Other useful options\n",
    "    auto_stack=True,\n",
    "    excluded_model_types=['KNN'],  # Exclude specific models\n",
    ")\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "We could also do:\n",
    "\n",
    "hyperparameters={\n",
    "    'GBM': {'num_leaves': 128, 'learning_rate': 0.05},\n",
    "    'CAT': {'iterations': 1000},\n",
    "}\n",
    "\n",
    "Why exclude KNN?\n",
    "\n",
    "- Very slow on large datasets\n",
    "- Often less performant than others\n",
    "- High memory consumption\n",
    "We could also exclude Linear Regression.\n",
    "\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

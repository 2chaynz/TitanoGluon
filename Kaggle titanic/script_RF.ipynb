{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_2423/3807874001.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_x['Age'].fillna(train_x['Age'].median(), inplace=True)\n",
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_2423/3807874001.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_x['Fare'].fillna(train_x['Fare'].median(), inplace=True)\n",
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_2423/3807874001.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_x['Age'].fillna(test_x['Age'].median(), inplace=True)\n",
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_2423/3807874001.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_x['Fare'].fillna(test_x['Fare'].median(), inplace=True)\n",
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_2423/3807874001.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_x['Embarked'].fillna(train_x['Embarked'].mode()[0], inplace=True)\n",
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_2423/3807874001.py:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_x['Embarked'].fillna(test_x['Embarked'].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.0s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.0s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.0s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   1.7s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=100; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=100; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=8, n_estimators=200; total time=   2.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=8, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   1.6s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=500; total time=   5.0s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=500; total time=   5.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=500; total time=   5.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=500; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   3.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   3.7s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=500; total time=   6.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   6.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   4.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   1.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.3s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.1s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   1.1s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   1.1s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   1.2s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   1.2s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   3.3s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   2.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   6.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   5.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   5.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   5.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   5.5s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=100; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   5.2s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   5.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   5.5s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   7.3s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   7.5s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   7.9s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   8.4s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   6.2s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   5.7s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   6.9s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   5.0s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.4s\n",
      "[CV] END max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   4.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   4.0s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   3.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   3.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=500; total time=   3.7s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
      "Best CV ROC-AUC score: 0.8709513860299894\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Titanic Kaggle Challenge - Random Forest\n",
    "\n",
    "\n",
    "For detailed explanations about metrics (ROC-AUC, precision, recall etc...), \n",
    "please refer to the Typst documentation file in the GitHub repository.\n",
    "\n",
    "This script implements:\n",
    "- Feature preprocessing and encoding\n",
    "- Random Forest classification\n",
    "- Hyperparameter tuning with RandomizedSearchCV\n",
    "- Cross-validation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#### DATA LOADING\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "train_y = train_data['Survived']\n",
    "train_x = train_data.drop(columns=['Survived'])\n",
    "\n",
    "#### FEATURE ENGINEERING\n",
    "\n",
    "# Drop columns that are not useful\n",
    "drop = ['Name', 'Ticket', 'Cabin']\n",
    "train_x = train_x.drop(columns=drop)\n",
    "test_x = test_data.drop(columns=drop)\n",
    "\n",
    "#### MISSING VALUES IMPUTATION\n",
    "\n",
    "\"\"\"\n",
    "General rule for imputation:\n",
    "- Numerical columns → replace with median (less sensitive to outliers than mean)\n",
    "- Categorical columns → replace with mode (most frequent value) or \"Unknown\"\n",
    "\"\"\"\n",
    "\n",
    "# Numerical columns\n",
    "train_x['Age'].fillna(train_x['Age'].median(), inplace=True)\n",
    "train_x['Fare'].fillna(train_x['Fare'].median(), inplace=True)\n",
    "test_x['Age'].fillna(test_x['Age'].median(), inplace=True)\n",
    "test_x['Fare'].fillna(test_x['Fare'].median(), inplace=True)\n",
    "\n",
    "# Others \n",
    "# .mode()[0] returns the most frequent value as a scalar (not a Series)\n",
    "train_x['Embarked'].fillna(train_x['Embarked'].mode()[0], inplace=True)\n",
    "test_x['Embarked'].fillna(test_x['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "#### ENCODING\n",
    "\n",
    "\"\"\"\n",
    "Encoding strategy:\n",
    "- Sex: OneHotEncoding → male (0/1), female (1/0)\n",
    "  Avoids implicit ordering (male=1, female=0 would suggest male > female)\n",
    "  \n",
    "- Embarked: OrdinalEncoding → Q=0, C=1, S=2\n",
    "  I assume an order from least to most affluent port\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# OneHot encoding for Sex\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "train_sex = pd.DataFrame(\n",
    "    ohe.fit_transform(train_x[['Sex']]), \n",
    "    columns=ohe.get_feature_names_out(['Sex'])\n",
    ")\n",
    "test_sex = pd.DataFrame(\n",
    "    ohe.transform(test_x[['Sex']]), \n",
    "    columns=ohe.get_feature_names_out(['Sex'])\n",
    ")\n",
    "\n",
    "# Drop original Sex column and reset index to avoid misalignment\n",
    "train_x = train_x.drop(columns=['Sex']).reset_index(drop=True)\n",
    "test_x = test_x.drop(columns=['Sex']).reset_index(drop=True)\n",
    "\n",
    "# Concatenate encoded Sex columns\n",
    "train_x = pd.concat([train_x, train_sex], axis=1)\n",
    "test_x = pd.concat([test_x, test_sex], axis=1)\n",
    "\n",
    "# Ordinal encoding for Embarked (Q < C < S in terms of affluence)\n",
    "embarked_enc = OrdinalEncoder(categories=[['Q', 'C', 'S']])\n",
    "train_x['Embarked'] = embarked_enc.fit_transform(train_x[['Embarked']])\n",
    "test_x['Embarked'] = embarked_enc.transform(test_x[['Embarked']])\n",
    "\n",
    "#### HYPERPARAMETERS \n",
    "\n",
    "\"\"\"\n",
    "I use RandomizedSearchCV instead of GridSearchCV for efficiency.\n",
    "\n",
    "Why no train_test_split ? \n",
    "I don't split train.csv into train/validation because:\n",
    "1. Cross-validation already provides a reliable performance estimate\n",
    "2. We can always come back and add a local test set if CV score is suspicious\n",
    "\n",
    "An other possibility is :\n",
    "X_train, X_test_local, y_train, y_test_local = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42\n",
    ")\n",
    "Then use X_train for RandomizedSearchCV and X_test_local for final validation.\n",
    "\"\"\"\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(random_state=1) \n",
    "\n",
    "# Define hyperparameter \n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "#### RANDOMIZED SEARCH WITH CROSS-VALIDATION\n",
    "\n",
    "\"\"\"\n",
    "RandomizedSearchCV automatically:\n",
    "1. Keeps the best combination based on ROC-AUC score\n",
    "2. Keeps a final model with best parameters on all of train_x \n",
    "\n",
    "The final model is stored in: random_search.  (random_search.best_estimator_)\n",
    "The best parameters are in: random_search.best_params_\n",
    "The best CV score is in: random_search.best_score_\n",
    "\"\"\"\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,              # Number of random combinations to test\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='roc_auc',      # Optimization metric\n",
    "    n_jobs=-1,              # Use all CPU cores\n",
    "    verbose=2,              # Display progress\n",
    "    random_state=1          # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model \n",
    "random_search.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best CV ROC-AUC score:\", random_search.best_score_)\n",
    "\n",
    "#### PREDICTIONS \n",
    "\n",
    "\"\"\"\n",
    "If the CV ROC-AUC score is satisfactory (typically > 0.80), proceed to prediction.\n",
    "If not, consider more (Feature engineering,Different encoding strategies,Try other models...)\n",
    "\"\"\"\n",
    "\n",
    "# Make predictions using the best model (automatically retrained on full train_x)\n",
    "predictions = random_search.predict(test_x)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "submission.to_csv('submission_rf.csv', index=False)\n",
    "\n",
    "''' \n",
    "OUTPUT :  Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
    "          Best CV ROC-AUC score: 0.8709513860299894\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
